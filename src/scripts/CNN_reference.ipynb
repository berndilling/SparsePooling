{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# downloads data at first execution\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "(x_c_train, y_c_train), (x_c_test, y_c_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(x):\n",
    "    #x = x.astype('float32')/255\n",
    "    #x = np.piecewise(x, [x <= 0.04045, x > 0.04045], \n",
    "    #                    [lambda x: x/12.92, lambda x: ((x + .055)/1.055)**2.4])\n",
    "    return .2126 * x[:,:,:,0] + .7152 * x[:,:,:,1]  + .07152 * x[:,:,:,2]\n",
    "\n",
    "def downsample(x):\n",
    "    return sum([x[i::2,j::2,:] for i in range(2) for j in range(2)])/4\n",
    "\n",
    "x_c_train = grayscale(x_c_train)\n",
    "x_c_test = grayscale(x_c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "y_c_train = keras.utils.to_categorical(y_c_train)\n",
    "y_c_test = keras.utils.to_categorical(y_c_test)\n",
    "\n",
    "x_train = x_train/np.max(x_train)\n",
    "x_test = x_test/np.max(x_test)\n",
    "x_c_train = x_c_train/np.max(x_c_train)\n",
    "x_c_test = x_c_test/np.max(x_c_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 28, ydim = 28,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compilemodel(model, optimizer):\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "def convnet(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dropout(.5))\n",
    "    input_shape = (None,)\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model\n",
    "\n",
    "def convnet1(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(128, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model\n",
    "\n",
    "def convnet2(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(128, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dropout(.5))\n",
    "    input_shape = (None,)\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_convnet(x_train, x_test, y_train, y_test, batch_size, epochs, \n",
    "                activation_function = 'relu', batch_norm = False, input_shape = (28,28,1)):\n",
    "    model_conv = convnet(input_shape = input_shape, num_classes = y_test.shape[1], \n",
    "                         activation_function = activation_function, batch_norm = batch_norm)\n",
    "    model_conv.summary()\n",
    "    x_train_conv = x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "    x_test_conv = x_test.reshape(x_test.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "    t = time.time()\n",
    "    history_conv = model_conv.fit(x_train_conv, y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs= epochs,\n",
    "                                  verbose=1,\n",
    "                                  validation_data=(x_test_conv, y_test))\n",
    "    elapsed_conv = time.time() - t\n",
    "    return model_conv, history_conv, elapsed_conv\n",
    "\n",
    "def cont_fit_convent(model_conv, x_train, x_test, y_train, y_test, batch_size, epochs, \n",
    "                     input_shape = (28,28,1)):\n",
    "    x_train_conv = x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "    x_test_conv = x_test.reshape(x_test.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "    t = time.time()\n",
    "    history_conv = model_conv.fit(x_train_conv, y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs= epochs,\n",
    "                                  verbose=1,\n",
    "                                  validation_data=(x_test_conv, y_test))\n",
    "    elapsed_conv = time.time() - t\n",
    "    return model_conv, history_conv, elapsed_conv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 52s 865us/step - loss: 0.3015 - acc: 0.9073 - val_loss: 0.0659 - val_acc: 0.9796\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 46s 764us/step - loss: 0.1010 - acc: 0.9693 - val_loss: 0.0460 - val_acc: 0.9849\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 41s 676us/step - loss: 0.0757 - acc: 0.9775 - val_loss: 0.0333 - val_acc: 0.9890\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 41s 691us/step - loss: 0.0632 - acc: 0.9810 - val_loss: 0.0309 - val_acc: 0.9887\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 41s 682us/step - loss: 0.0538 - acc: 0.9838 - val_loss: 0.0276 - val_acc: 0.9909\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 37s 622us/step - loss: 0.0477 - acc: 0.9849 - val_loss: 0.0297 - val_acc: 0.9899\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 37s 612us/step - loss: 0.0422 - acc: 0.9871 - val_loss: 0.0293 - val_acc: 0.9907\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 37s 618us/step - loss: 0.0388 - acc: 0.9882 - val_loss: 0.0254 - val_acc: 0.9919\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 37s 609us/step - loss: 0.0354 - acc: 0.9891 - val_loss: 0.0256 - val_acc: 0.9924\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 38s 636us/step - loss: 0.0330 - acc: 0.9902 - val_loss: 0.0253 - val_acc: 0.9919\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model_conv, history_conv, t_elapsed_conv = fit_convnet(x_train, x_test, y_train, y_test, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 315,146\n",
      "Trainable params: 315,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1.8818 - acc: 0.3145 - val_loss: 1.5983 - val_acc: 0.4419\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 50s 1ms/step - loss: 1.5777 - acc: 0.4395 - val_loss: 1.4237 - val_acc: 0.5038\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 46s 929us/step - loss: 1.4529 - acc: 0.4889 - val_loss: 1.2967 - val_acc: 0.5518\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 45s 900us/step - loss: 1.3542 - acc: 0.5248 - val_loss: 1.2067 - val_acc: 0.5809\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 49s 981us/step - loss: 1.2990 - acc: 0.5437 - val_loss: 1.1624 - val_acc: 0.5987\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 46s 924us/step - loss: 1.2494 - acc: 0.5638 - val_loss: 1.1038 - val_acc: 0.6188\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 43s 857us/step - loss: 1.2027 - acc: 0.5812 - val_loss: 1.0835 - val_acc: 0.6290\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 42s 837us/step - loss: 1.1666 - acc: 0.5933 - val_loss: 1.0388 - val_acc: 0.6429\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 43s 858us/step - loss: 1.1372 - acc: 0.6040 - val_loss: 1.0505 - val_acc: 0.6398\n",
      "Epoch 10/20\n",
      "35328/50000 [====================>.........] - ETA: 13s - loss: 1.1122 - acc: 0.6144"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "model_conv_c, history_conv_c, t_elapsed_conv_c = fit_convnet(x_c_train, x_c_test, y_c_train, y_c_test, \n",
    "                                                            batch_size, epochs, input_shape = (32,32,1))\n",
    "\n",
    "#model_conv_c, history_conv_c, t_elapsed_conv_c = cont_fit_convent(model_conv_c, x_c_train, x_c_test, \n",
    "#                                                                  y_c_train, y_c_test, \n",
    "#                                                                  batch_size, epochs, input_shape = (32,32,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
