{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# downloads data at first execution\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "(x_c_train, y_c_train), (x_c_test, y_c_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(x):\n",
    "    #x = x.astype('float32')/255\n",
    "    #x = np.piecewise(x, [x <= 0.04045, x > 0.04045], \n",
    "    #                    [lambda x: x/12.92, lambda x: ((x + .055)/1.055)**2.4])\n",
    "    return .2126 * x[:,:,:,0] + .7152 * x[:,:,:,1]  + .07152 * x[:,:,:,2]\n",
    "\n",
    "def downsample(x):\n",
    "    return sum([x[i::2,j::2,:] for i in range(2) for j in range(2)])/4\n",
    "\n",
    "x_c_train = grayscale(x_c_train)\n",
    "x_c_test = grayscale(x_c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "y_c_train = keras.utils.to_categorical(y_c_train)\n",
    "y_c_test = keras.utils.to_categorical(y_c_test)\n",
    "\n",
    "x_train = x_train/np.max(x_train)\n",
    "x_test = x_test/np.max(x_test)\n",
    "x_c_train = x_c_train/np.max(x_c_train)\n",
    "x_c_test = x_c_test/np.max(x_c_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 28, ydim = 28,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compilemodel(model, optimizer):\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "def convnet(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dropout(.5))\n",
    "    input_shape = (None,)\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model\n",
    "\n",
    "def convnet1(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(128, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model\n",
    "\n",
    "def convnet2(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(128, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(512, activation=activation_function))\n",
    "    model.add(Dropout(.25))\n",
    "    input_shape = (None,)\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model\n",
    "\n",
    "def convnet3(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (10, 10), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (10, 10), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dropout(.5))\n",
    "    input_shape = (None,)\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_convnet(x_train, x_test, y_train, y_test, batch_size, epochs, \n",
    "                activation_function = 'relu', batch_norm = False, input_shape = (28,28,1)):\n",
    "    model_conv = convnet1(input_shape = input_shape, num_classes = y_test.shape[1], \n",
    "                         activation_function = activation_function, batch_norm = batch_norm)\n",
    "    model_conv.summary()\n",
    "    x_train_conv = x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "    x_test_conv = x_test.reshape(x_test.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "    t = time.time()\n",
    "    history_conv = model_conv.fit(x_train_conv, y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs= epochs,\n",
    "                                  verbose=1,\n",
    "                                  validation_data=(x_test_conv, y_test))\n",
    "    elapsed_conv = time.time() - t\n",
    "    return model_conv, history_conv, elapsed_conv\n",
    "\n",
    "def cont_fit_convent(model_conv, x_train, x_test, y_train, y_test, batch_size, epochs, \n",
    "                     input_shape = (28,28,1)):\n",
    "    x_train_conv = x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "    x_test_conv = x_test.reshape(x_test.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "    t = time.time()\n",
    "    history_conv = model_conv.fit(x_train_conv, y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs= epochs,\n",
    "                                  verbose=1,\n",
    "                                  validation_data=(x_test_conv, y_test))\n",
    "    elapsed_conv = time.time() - t\n",
    "    return model_conv, history_conv, elapsed_conv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              132096    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 235,018\n",
      "Trainable params: 235,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.4553 - acc: 0.8539 - val_loss: 0.0993 - val_acc: 0.9696\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1530 - acc: 0.9549 - val_loss: 0.0653 - val_acc: 0.9794\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.1150 - acc: 0.9655 - val_loss: 0.0530 - val_acc: 0.9844\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.0965 - acc: 0.9715 - val_loss: 0.0485 - val_acc: 0.9866\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.0807 - acc: 0.9760 - val_loss: 0.0448 - val_acc: 0.9880\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0712 - acc: 0.9789 - val_loss: 0.0481 - val_acc: 0.9861\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0630 - acc: 0.9814 - val_loss: 0.0426 - val_acc: 0.9873\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0571 - acc: 0.9826 - val_loss: 0.0388 - val_acc: 0.9890\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.0558 - acc: 0.9826 - val_loss: 0.0422 - val_acc: 0.9880\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.0488 - acc: 0.9855 - val_loss: 0.0372 - val_acc: 0.9902\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model_conv, history_conv, t_elapsed_conv = fit_convnet(x_train, x_test, y_train, y_test, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 97,802\n",
      "Trainable params: 97,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.9253 - acc: 0.2950 - val_loss: 1.6176 - val_acc: 0.4261\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 1.6202 - acc: 0.4228 - val_loss: 1.4947 - val_acc: 0.4807\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 1.5016 - acc: 0.4697 - val_loss: 1.3952 - val_acc: 0.5194\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 1.4246 - acc: 0.5014 - val_loss: 1.3039 - val_acc: 0.5500\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 4s 78us/step - loss: 1.3671 - acc: 0.5196 - val_loss: 1.2486 - val_acc: 0.5725\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 1.3143 - acc: 0.5406 - val_loss: 1.2295 - val_acc: 0.5744\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.2750 - acc: 0.5563 - val_loss: 1.1620 - val_acc: 0.5977\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 1.2433 - acc: 0.5652 - val_loss: 1.1504 - val_acc: 0.6064\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 1.2180 - acc: 0.5759 - val_loss: 1.1122 - val_acc: 0.6194\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 1.1923 - acc: 0.5865 - val_loss: 1.1159 - val_acc: 0.6207\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 1.1680 - acc: 0.5943 - val_loss: 1.0923 - val_acc: 0.6251\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 1.1474 - acc: 0.6020 - val_loss: 1.0552 - val_acc: 0.6373\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 1.1285 - acc: 0.6078 - val_loss: 1.0315 - val_acc: 0.6464\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 1.1151 - acc: 0.6129 - val_loss: 1.0458 - val_acc: 0.6354\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 1.0942 - acc: 0.6205 - val_loss: 1.0368 - val_acc: 0.6427\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 1.0808 - acc: 0.6245 - val_loss: 1.0078 - val_acc: 0.6511\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 1.0650 - acc: 0.6310 - val_loss: 0.9897 - val_acc: 0.6562\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 1.0520 - acc: 0.6350 - val_loss: 0.9996 - val_acc: 0.6588\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 1.0391 - acc: 0.6376 - val_loss: 0.9804 - val_acc: 0.6647\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 1.0235 - acc: 0.6462 - val_loss: 0.9841 - val_acc: 0.6679\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 1.0135 - acc: 0.6450 - val_loss: 0.9608 - val_acc: 0.6716\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 1.0017 - acc: 0.6520 - val_loss: 0.9429 - val_acc: 0.6798\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.9867 - acc: 0.6574 - val_loss: 0.9482 - val_acc: 0.6766\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.9770 - acc: 0.6604 - val_loss: 0.9584 - val_acc: 0.6714\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.9673 - acc: 0.6624 - val_loss: 0.9283 - val_acc: 0.6831\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.9595 - acc: 0.6675 - val_loss: 0.9323 - val_acc: 0.6797\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.9482 - acc: 0.6724 - val_loss: 0.9291 - val_acc: 0.6817\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 0.9410 - acc: 0.6727 - val_loss: 0.9294 - val_acc: 0.6836\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.9278 - acc: 0.6764 - val_loss: 0.8926 - val_acc: 0.6965\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 0.9292 - acc: 0.6736 - val_loss: 0.8972 - val_acc: 0.6921\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.9146 - acc: 0.6809 - val_loss: 0.8987 - val_acc: 0.6921\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.9088 - acc: 0.6828 - val_loss: 0.9094 - val_acc: 0.6891\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.8994 - acc: 0.6862 - val_loss: 0.8796 - val_acc: 0.7017\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 0.8899 - acc: 0.6880 - val_loss: 0.9290 - val_acc: 0.6853\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.8848 - acc: 0.6907 - val_loss: 0.8914 - val_acc: 0.6953\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 0.8823 - acc: 0.6919 - val_loss: 0.8812 - val_acc: 0.7001\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.8694 - acc: 0.6986 - val_loss: 0.8720 - val_acc: 0.7037\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 0.8629 - acc: 0.6974 - val_loss: 0.8953 - val_acc: 0.6929\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.8545 - acc: 0.7002 - val_loss: 0.8563 - val_acc: 0.7050\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.8564 - acc: 0.6994 - val_loss: 0.8588 - val_acc: 0.7084\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 4s 78us/step - loss: 0.8430 - acc: 0.7028 - val_loss: 0.8719 - val_acc: 0.7033\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 4s 78us/step - loss: 0.8446 - acc: 0.7045 - val_loss: 0.8710 - val_acc: 0.7030\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 0.8284 - acc: 0.7107 - val_loss: 0.8407 - val_acc: 0.7133\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.8284 - acc: 0.7122 - val_loss: 0.8698 - val_acc: 0.7077\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 0.8281 - acc: 0.7093 - val_loss: 0.8685 - val_acc: 0.7032\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 4s 78us/step - loss: 0.8254 - acc: 0.7123 - val_loss: 0.8714 - val_acc: 0.7011\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 0.8113 - acc: 0.7163 - val_loss: 0.8465 - val_acc: 0.7118\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 0.8088 - acc: 0.7158 - val_loss: 0.8541 - val_acc: 0.7067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.8033 - acc: 0.7195 - val_loss: 0.8534 - val_acc: 0.7151\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.7996 - acc: 0.7203 - val_loss: 0.8355 - val_acc: 0.7155\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.7973 - acc: 0.7209 - val_loss: 0.8587 - val_acc: 0.7007\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7938 - acc: 0.7224 - val_loss: 0.8361 - val_acc: 0.7178\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 0.7876 - acc: 0.7248 - val_loss: 0.8249 - val_acc: 0.7248\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 0.7842 - acc: 0.7239 - val_loss: 0.8315 - val_acc: 0.7169\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 0.7815 - acc: 0.7254 - val_loss: 0.8292 - val_acc: 0.7216\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 0.7788 - acc: 0.7259 - val_loss: 0.8344 - val_acc: 0.7142\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.7630 - acc: 0.7325 - val_loss: 0.8422 - val_acc: 0.7161\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 0.7702 - acc: 0.7292 - val_loss: 0.8337 - val_acc: 0.7170\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 0.7625 - acc: 0.7326 - val_loss: 0.8217 - val_acc: 0.7198\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 4s 78us/step - loss: 0.7621 - acc: 0.7319 - val_loss: 0.8333 - val_acc: 0.7196\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7586 - acc: 0.7339 - val_loss: 0.8167 - val_acc: 0.7255\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7560 - acc: 0.7329 - val_loss: 0.8235 - val_acc: 0.7179\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.7476 - acc: 0.7375 - val_loss: 0.8327 - val_acc: 0.7152\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7498 - acc: 0.7363 - val_loss: 0.8323 - val_acc: 0.7167\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.7429 - acc: 0.7380 - val_loss: 0.8306 - val_acc: 0.7188\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7386 - acc: 0.7411 - val_loss: 0.8283 - val_acc: 0.7218\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7423 - acc: 0.7390 - val_loss: 0.8308 - val_acc: 0.7178\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.7394 - acc: 0.7398 - val_loss: 0.8257 - val_acc: 0.7204\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.7354 - acc: 0.7413 - val_loss: 0.8211 - val_acc: 0.7244\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7316 - acc: 0.7418 - val_loss: 0.8229 - val_acc: 0.7174\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7300 - acc: 0.7434 - val_loss: 0.8295 - val_acc: 0.7155\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7214 - acc: 0.7451 - val_loss: 0.8169 - val_acc: 0.7250\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7185 - acc: 0.7467 - val_loss: 0.8473 - val_acc: 0.7132\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 0.7194 - acc: 0.7451 - val_loss: 0.8276 - val_acc: 0.7198\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.7190 - acc: 0.7472 - val_loss: 0.8185 - val_acc: 0.7229\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.7148 - acc: 0.7472 - val_loss: 0.8211 - val_acc: 0.7234\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.7072 - acc: 0.7487 - val_loss: 0.8139 - val_acc: 0.7251\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.7073 - acc: 0.7503 - val_loss: 0.8307 - val_acc: 0.7200\n",
      "Epoch 79/100\n",
      " 2432/50000 [>.............................] - ETA: 3s - loss: 0.6697 - acc: 0.7644"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "model_conv_c, history_conv_c, t_elapsed_conv_c = fit_convnet(x_c_train, x_c_test, y_c_train, y_c_test, \n",
    "                                                            batch_size, epochs, input_shape = (32,32,1))\n",
    "\n",
    "#model_conv_c, history_conv_c, t_elapsed_conv_c = cont_fit_convent(model_conv_c, x_c_train, x_c_test, \n",
    "#                                                                  y_c_train, y_c_test, \n",
    "#                                                                  batch_size, epochs, input_shape = (32,32,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
