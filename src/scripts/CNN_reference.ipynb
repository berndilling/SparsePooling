{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# downloads data at first execution\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "(x_c_train, y_c_train), (x_c_test, y_c_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(x):\n",
    "    #x = x.astype('float32')/255\n",
    "    #x = np.piecewise(x, [x <= 0.04045, x > 0.04045], \n",
    "    #                    [lambda x: x/12.92, lambda x: ((x + .055)/1.055)**2.4])\n",
    "    return .2126 * x[:,:,:,0] + .7152 * x[:,:,:,1]  + .07152 * x[:,:,:,2]\n",
    "\n",
    "def downsample(x):\n",
    "    return sum([x[i::2,j::2,:] for i in range(2) for j in range(2)])/4\n",
    "\n",
    "x_c_train = grayscale(x_c_train)\n",
    "x_c_test = grayscale(x_c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "y_c_train = keras.utils.to_categorical(y_c_train)\n",
    "y_c_test = keras.utils.to_categorical(y_c_test)\n",
    "\n",
    "x_train = x_train/np.max(x_train)\n",
    "x_test = x_test/np.max(x_test)\n",
    "x_c_train = x_c_train/np.max(x_c_train)\n",
    "x_c_test = x_c_test/np.max(x_c_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 28, ydim = 28,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compilemodel(model, optimizer):\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "def convnet(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dropout(.5))\n",
    "    input_shape = (None,)\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model\n",
    "\n",
    "def convnet1(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(128, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    #model.add(Dropout(.25))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model\n",
    "\n",
    "def convnet2(input_shape = (28,28,1), num_classes = 10, optimizer = Adam(), activation_function = 'relu', batch_norm = False):\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))  \n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_function, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(128, (3, 3), activation=activation_function))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Flatten())\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    \n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dropout(.5))\n",
    "    input_shape = (None,)\n",
    "    if batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    compilemodel(model, optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_convnet(x_train, x_test, y_train, y_test, batch_size, epochs, \n",
    "                activation_function = 'relu', batch_norm = False, input_shape = (28,28,1)):\n",
    "    model_conv = convnet1(input_shape = input_shape, num_classes = y_test.shape[1], \n",
    "                         activation_function = activation_function, batch_norm = batch_norm)\n",
    "    model_conv.summary()\n",
    "    x_train_conv = x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "    x_test_conv = x_test.reshape(x_test.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "    t = time.time()\n",
    "    history_conv = model_conv.fit(x_train_conv, y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs= epochs,\n",
    "                                  verbose=1,\n",
    "                                  validation_data=(x_test_conv, y_test))\n",
    "    elapsed_conv = time.time() - t\n",
    "    return model_conv, history_conv, elapsed_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 93,962\n",
      "Trainable params: 93,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 38s 635us/step - loss: 0.2934 - acc: 0.9143 - val_loss: 0.0848 - val_acc: 0.9754\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 38s 630us/step - loss: 0.0881 - acc: 0.9729 - val_loss: 0.0720 - val_acc: 0.9778\n",
      "Epoch 3/10\n",
      "34560/60000 [================>.............] - ETA: 15s - loss: 0.0653 - acc: 0.9790"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model_conv, history_conv, t_elapsed_conv = fit_convnet(x_train, x_test, y_train, y_test, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 315,146\n",
      "Trainable params: 315,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 41s 813us/step - loss: 1.8489 - acc: 0.3296 - val_loss: 1.5290 - val_acc: 0.4605\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 42s 848us/step - loss: 1.5439 - acc: 0.4569 - val_loss: 1.3675 - val_acc: 0.5294\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 39s 787us/step - loss: 1.4111 - acc: 0.5043 - val_loss: 1.2678 - val_acc: 0.5613\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 40s 802us/step - loss: 1.3187 - acc: 0.5403 - val_loss: 1.1727 - val_acc: 0.5926\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 41s 817us/step - loss: 1.2558 - acc: 0.5665 - val_loss: 1.1351 - val_acc: 0.6070\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 40s 809us/step - loss: 1.2093 - acc: 0.5793 - val_loss: 1.1130 - val_acc: 0.6098\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 40s 809us/step - loss: 1.1729 - acc: 0.5947 - val_loss: 1.0633 - val_acc: 0.6349\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 41s 810us/step - loss: 1.1369 - acc: 0.6040 - val_loss: 1.0702 - val_acc: 0.6301\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 39s 788us/step - loss: 1.1084 - acc: 0.6137 - val_loss: 1.0033 - val_acc: 0.6553\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 40s 804us/step - loss: 1.0829 - acc: 0.6262 - val_loss: 1.0082 - val_acc: 0.6464\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 41s 811us/step - loss: 1.0610 - acc: 0.6309 - val_loss: 0.9783 - val_acc: 0.6655\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 41s 819us/step - loss: 1.0365 - acc: 0.6411 - val_loss: 0.9476 - val_acc: 0.6739\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 41s 816us/step - loss: 1.0140 - acc: 0.6483 - val_loss: 0.9500 - val_acc: 0.6760\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 41s 821us/step - loss: 1.0008 - acc: 0.6516 - val_loss: 0.9385 - val_acc: 0.6790\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 40s 809us/step - loss: 0.9871 - acc: 0.6563 - val_loss: 0.9334 - val_acc: 0.6789\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 40s 808us/step - loss: 0.9689 - acc: 0.6623 - val_loss: 0.9281 - val_acc: 0.6809\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 40s 800us/step - loss: 0.9568 - acc: 0.6674 - val_loss: 0.9169 - val_acc: 0.6881\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 40s 800us/step - loss: 0.9406 - acc: 0.6737 - val_loss: 0.9028 - val_acc: 0.6896\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 40s 804us/step - loss: 0.9355 - acc: 0.6748 - val_loss: 0.8959 - val_acc: 0.6917\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 40s 798us/step - loss: 0.9147 - acc: 0.6823 - val_loss: 0.9018 - val_acc: 0.6890\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "model_conv_c, history_conv_c, t_elapsed_conv_c = fit_convnet(x_c_train, x_c_test, y_c_train, y_c_test, batch_size, epochs, input_shape = (32,32,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
